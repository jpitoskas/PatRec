{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>Αναγνώριση Προτύπων - 2η Εργαστηριακή Άσκηση</h1> </center>\n",
    "\n",
    "---\n",
    "\n",
    "<center> <h2>Θέμα: Αναγνώριση φωνής με Κρυφά Μαρκοβιανά Μοντέλα και Αναδρομικά Νευρωνικά Δίκτυα</h2> </center>\n",
    "<center> <h2>ΣΧΟΛΗ: ΣΗΜΜΥ</h2> </center>\n",
    "<img src=\"pyrforos-digamma.png\" width=\"100\">\n",
    "\n",
    "Ονοματεπώνυμο | Αριθμός Μητρώου\n",
    "------------ | -------------\n",
    "Γιάννης Πιτόσκας | 03115077\n",
    "Αντώνης Παπαοικονόμου | 03115140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /usr/local/lib/python3.7/site-packages (19.3.1)\n",
      "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.7/site-packages (0.22)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.17.4)\n",
      "Collecting numpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/14/f71a89e03578084111e352f464d9f3b7f701ebbecbd1a60e89c96983ef77/numpy-1.18.0-cp37-cp37m-macosx_10_9_x86_64.whl (15.1MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1MB 5.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Found existing installation: numpy 1.17.4\n",
      "    Uninstalling numpy-1.17.4:\n",
      "      Successfully uninstalled numpy-1.17.4\n",
      "Successfully installed numpy-1.18.0\n",
      "Requirement already up-to-date: joblib in /usr/local/lib/python3.7/site-packages (0.14.1)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.7/site-packages (0.7.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/site-packages (from librosa) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/site-packages (from librosa) (0.14.1)\n",
      "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/site-packages (from librosa) (0.46.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/site-packages (from librosa) (1.18.0)\n",
      "Requirement already satisfied: six>=1.3 in /Users/antonypap/Library/Python/3.7/lib/python/site-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/site-packages (from librosa) (4.4.0)\n",
      "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/site-packages (from librosa) (2.1.8)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/site-packages (from librosa) (0.22)\n",
      "Requirement already satisfied: llvmlite>=0.30.0dev0 in /usr/local/lib/python3.7/site-packages (from numba>=0.43.0->librosa) (0.30.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa) (1.13.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.19)\n",
      "Collecting pomegranate\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/b7/4b1edd698b0b49dec2024e9a6971998f4e9d1fe974f65410c2005e1f82e6/pomegranate-0.12.0-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (7.6MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6MB 5.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.9.0b4 in /usr/local/lib/python3.7/site-packages (from pomegranate) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.7/site-packages (from pomegranate) (1.18.0)\n",
      "Collecting pyyaml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/c9/e5be955a117a1ac548cdd31e37e8fd7b02ce987f9655f5c7563c656d5dcb/PyYAML-5.2.tar.gz (265kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 6.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/site-packages (from pomegranate) (1.3.1)\n",
      "Collecting networkx>=2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 6.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/site-packages (from networkx>=2.0->pomegranate) (4.4.0)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.2-cp37-cp37m-macosx_10_14_x86_64.whl size=151474 sha256=d2f247ffacfafe5319da34f48dcdb67dd0d634dfada61e985ea89fed15ac0412\n",
      "  Stored in directory: /Users/antonypap/Library/Caches/pip/wheels/54/b7/c7/2ada654ee54483c9329871665aaf4a6056c3ce36f29cf66e67\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, networkx, pomegranate\n",
      "Successfully installed networkx-2.4 pomegranate-0.12.0 pyyaml-5.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install scikit-learn --upgrade\n",
    "!pip3 install numpy --upgrade\n",
    "!pip3 install --upgrade joblib\n",
    "!pip3 install librosa\n",
    "!pip3 install pomegranate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για το διάβασμα των δεδομένων του [Free Spoken Digits Dataset](https://github.com/Jakobovski/free-spoken-digit-dataset) μέσω της τροποποιημένης συνάρτησης ```parser()``` έχουμε:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def parser(directory):\n",
    "    # Parse relevant dataset info\n",
    "    files = glob(os.path.join(directory, '*.wav'))\n",
    "    fnames = [f.rsplit(str(os.path.sep), 1)[1].split('.')[0].split('_') for f in files]\n",
    "    ids = [f[2] for f in fnames]\n",
    "    y = [int(f[0]) for f in fnames]\n",
    "    speakers = [f[1] for f in fnames]\n",
    "    _, Fs = librosa.core.load(files[0], sr=None)\n",
    "\n",
    "    def read_wav(f):\n",
    "        global Fs\n",
    "        wav, fs = librosa.core.load(f, sr=None)\n",
    "        return wav\n",
    "\n",
    "    # Read all wavs\n",
    "    wavs = [read_wav(f) for f in files]\n",
    "\n",
    "    # Extract MFCCs for all wavs\n",
    "    window = 30 * Fs // 1000\n",
    "    step = window // 2\n",
    "    frames = [librosa.feature.mfcc(wav, Fs, n_fft=window, hop_length=window - step, n_mfcc=6).T for wav in wavs]\n",
    "    # Print dataset info\n",
    "    print('Total wavs: {}'.format(len(frames)))\n",
    "\n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate(frames))\n",
    "    for i in range(len(frames)):\n",
    "        frames[i] = scaler.transform(frames[i])\n",
    "\n",
    "    # Split to train-test\n",
    "    X_train, y_train, spk_train = [], [], []\n",
    "    X_test, y_test, spk_test = [], [], []\n",
    "    test_indices = ['0', '1', '2', '3', '4']\n",
    "    for idx, frame, label, spk in zip(ids, frames, y, speakers):\n",
    "        if str(idx) in test_indices:\n",
    "            X_test.append(frame)\n",
    "            y_test.append(label)\n",
    "            spk_test.append(spk)\n",
    "        else:\n",
    "            X_train.append(frame)\n",
    "            y_train.append(label)\n",
    "            spk_train.append(spk)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, spk_train, spk_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wavs: 2000\n"
     ]
    }
   ],
   "source": [
    "record_dir = os.path.abspath(\"./free-spoken-digit-dataset-master/recordings/\")\n",
    "\n",
    "X, X_test, y, y_test, spk_train, spk_test = parser(record_dir)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 9\n",
    "Χωρίζουμε τα `train` δεδομένα σε νέα `train` και `validation set` με ποσοστό $80\\%$ - $20\\%$ αντίστοιχα, χρησιμοποιώντας `stratified split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 10\n",
    "Φτιάχνουμε ένα python dicitonary το οποίο έχει για κάθε ψηφίο όλα τα δεδομένα που υπάρχουν στο train set και αντιστοιχούν στο ψηφίο αυτό και στην συνέχεια φτιάχνουμε τους πίνακες μετάβασης για τα `HMMs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4672, 6)\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from pomegranate import *\n",
    "import numpy as np\n",
    "\n",
    "X_digit = {digit : np.concatenate(X_train[y_train==digit]) for digit in range(10)}\n",
    "\n",
    "# X0 = X[0]\n",
    "print(X_digit[0].shape)\n",
    "\n",
    "n_states = 4 # the number of HMM states\n",
    "n_mixtures = 2 # the number of Gaussians\n",
    "    \n",
    "trans_mat = np.diag(np.ones(n_states-1), k=1) # your transition matrix\n",
    "starts = [1] + [0]*(n_states-1) # your starting probability matrix\n",
    "ends = [0]*(n_states-1) + [1] # your ending probability matrix\n",
    "\n",
    "print(trans_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 11\n",
    "Για το train των μοντέλων με χρήση του αλγόριθμου `Expectation Mazimization` έχουμε:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12045875 -1.0990843   0.90073854 -0.14875363  1.758961    1.4135393 ]\n",
      "-inf\n"
     ]
    }
   ],
   "source": [
    "dists = [] # list of probability distributions for the HMM states\n",
    "for i in range(n_states):\n",
    "    a = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, n_mixtures, X_digit[0])\n",
    "    dists.append(a)\n",
    "\n",
    "data = X_digit[0] # your data: must be a Python list that contains: 2D lists with the sequences (so its dimension would be num_sequences x seq_length x feature_dimension)\n",
    "          # But be careful, it is not a numpy array, it is a Python list (so each sequence can have different length)\n",
    "\n",
    "print(data[0])\n",
    "    \n",
    "# Define the GMM-HMM\n",
    "model = HiddenMarkovModel.from_matrix(trans_mat, dists, starts, ends, state_names=['s{}'.format(i) for i in range(n_states)])\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data, max_iterations=5)\n",
    "\n",
    "# Predict a sequence\n",
    "sample = X_val[0] # a sample sequence\n",
    "# print(sample[0])\n",
    "logp, _ = model.viterbi(sample) # Run viterbi algorithm and return log-probability\n",
    "print(logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 12\n",
    "Για την αξιολόγηση και την βελτιστοποίηση των παραμέτρων στο `validation set`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για την παραπάνω βελτιστοποίηση έχουμε για το `test set`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 13\n",
    "Για τα Confusion Matrices θα χρησιμοποιήσουμε την παρακάτω βοηθητική συνάρτηση:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 14\n",
    "1. Για το `LSTM` έχουμε:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FrameLevelDataset(Dataset):\n",
    "    def __init__(self, feats, labels):\n",
    "        \"\"\"\n",
    "            feats: Python list of numpy arrays that contain the sequence features.\n",
    "                   Each element of this list is a numpy array of shape seq_length x feature_dimension\n",
    "            labels: Python list that contains the label for each sequence (each label must be an integer)\n",
    "        \"\"\"\n",
    "#         self.lengths =  # Find the lengths \n",
    "\n",
    "        self.feats = self.zero_pad_and_stack(feats)\n",
    "        if isinstance(labels, (list, tuple)):\n",
    "            self.labels = np.array(labels).astype('int64')\n",
    "\n",
    "    def zero_pad_and_stack(self, x):\n",
    "        \"\"\"\n",
    "            This function performs zero padding on a list of features and forms them into a numpy 3D array\n",
    "            returns\n",
    "                padded: a 3D numpy array of shape num_sequences x max_sequence_length x feature_dimension\n",
    "        \"\"\"\n",
    "        padded = []\n",
    "        # --------------- Insert your code here ---------------- #\n",
    "\n",
    "        return padded\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.feats[item], self.labels[item], self.lengths[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "\n",
    "\n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, rnn_size, output_dim, num_layers, bidirectional=False):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.feature_size = rnn_size * 2 if self.bidirectional else rnn_size\n",
    "\n",
    "        # --------------- Insert your code here ---------------- #\n",
    "        # Initialize the LSTM, Dropout, Output layers\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\" \n",
    "            x : 3D numpy array of dimension N x L x D\n",
    "                N: batch index\n",
    "                L: sequence index\n",
    "                D: feature index\n",
    "\n",
    "            lengths: N x 1\n",
    "         \"\"\"\n",
    "        \n",
    "        # --------------- Insert your code here ---------------- #\n",
    "        \n",
    "        # You must have all of the outputs of the LSTM, but you need only the last one (that does not exceed the sequence length)\n",
    "        # To get it use the last_timestep method\n",
    "        # Then pass it through the remaining network\n",
    "\n",
    "        return last_outputs\n",
    "\n",
    "    def last_timestep(self, outputs, lengths, bidirectional=False):\n",
    "        \"\"\"\n",
    "            Returns the last output of the LSTM taking into account the zero padding\n",
    "        \"\"\"\n",
    "        if bidirectional:\n",
    "            forward, backward = self.split_directions(outputs)\n",
    "            last_forward = self.last_by_index(forward, lengths)\n",
    "            last_backward = backward[:, 0, :]\n",
    "            # Concatenate and return - maybe add more functionalities like average\n",
    "            return torch.cat((last_forward, last_backward), dim=-1)\n",
    "\n",
    "        else:\n",
    "            return self.last_by_index(outputs, lengths)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_directions(outputs):\n",
    "        direction_size = int(outputs.size(-1) / 2)\n",
    "        forward = outputs[:, :, :direction_size]\n",
    "        backward = outputs[:, :, direction_size:]\n",
    "        return forward, backward\n",
    "\n",
    "    @staticmethod\n",
    "    def last_by_index(outputs, lengths):\n",
    "        # Index of the last output for each sequence.\n",
    "        idx = (lengths - 1).view(-1, 1).expand(outputs.size(0),\n",
    "                                               outputs.size(2)).unsqueeze(1)\n",
    "        return outputs.gather(1, idx).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
